#!/usr/env/ pythonfrom rlagents.rl_agent.eligibility_traces_cql_2d1 import EligibilityTracesCQL2D1Agent__author__ = 'budikurniawan'import pickleimport timeimport datetimefrom simulation import MultiAgentSimulationfrom rl_util.qcache import QCachefrom rlagents.rl_agent.base_agent import BaseAgentfrom . import rl_utilsimport randomimport os"""Multi-agent simulation of one fighter aircraft and one RL-trained aircraft in a 1v1 configuration."""class RLMultiAgentSimulation(MultiAgentSimulation):    """    An RL simulator    """    def __init__(self, scenario=None, root_path=None):        if not scenario:            scenario = self.default_scenario()        super(RLMultiAgentSimulation, self).__init__(scenario, root_path)        self.scenario = scenario
    def run_multiple(self):        for i in range(rl_utils.num_trials):            random.seed(i)            self.random_seed = i            print("========= learning session " + str(i + 1))            self.run()                def run(self):        print("RLSimulation.run()")        scenario_file = rl_utils.scenario.split('/')[-1]        output_dir = 'rl_results/' + scenario_file + "/" + str(rl_utils.num_episodes).zfill(8) \                + "-" + str(rl_utils.MIN_NUM_IN_GOAL).zfill(4)        if not os.path.exists(output_dir):            os.makedirs(output_dir)        random_seed = str(self.random_seed)        q_filepath = output_dir + '/Q-' + random_seed        sorted_q_filepath = output_dir + '/sorted-q-' + random_seed + '.txt'        viper_trace = None        cobra_trace = None        num_episodes = rl_utils.num_episodes        q_cache = QCache()        # support for eligibility traces        e = {}        e.fromkeys(range(4000000))                self.timestep = 0        self.q = q_cache.get("q")        if rl_utils.test_only:            num_episodes = 1            q_cache.set_data(pickle.load(open(q_filepath)))            print("Test only. Cached data:", str(q_cache.data))
        start_time = time.time()        global_log = open('global-log.txt', 'w')        test_interval = 100 #if num_episodes < 5000 else 1000        for episode in range(1, num_episodes + 1):            self.learn(episode, q_cache, e, global_log)            # test every test_interval episodes            if episode % test_interval == 0:                print("====== Testing after episode " + str(episode) + " (" + str(self.random_seed) + ")")                num_episodes = rl_utils.num_episodes                test_result_filename = output_dir + "/test-result-" + str(self.random_seed).zfill(2) + "-" + str(episode).zfill(6) + ".txt"                if self.test(episode, q_cache, test_result_filename, global_log):                    pass #break        end_time = time.time()        duration = end_time - start_time        print("--------------------------------------------------------------------\n ---- Total duration (learning+activity): ", duration)        print("Total timesteps (excluding activity): ", self.timestep)                # Save Q and sorted-q files        self.serialize(q_cache.data, q_filepath)        f = open(sorted_q_filepath, 'w')        rl_utils.print_q(q_cache.data['q'], f) #action_length is assumed to be <40        f.close()
    def learn(self, episode, q_cache, e, global_log):        print("Learning episode " + str(episode) + " (" + str(self.random_seed) + ")")        num_episodes = rl_utils.num_episodes        logFilename = 'q_snapshots/q' + str(episode) + '.txt'        f = open(logFilename, 'w')        self.scenario['dt'] = rl_utils.dt        self.setup(self.scenario)        if isinstance(self.viper.pilot, BaseAgent):            viper_pilot = self.viper.pilot            viper_pilot.set_episode(episode)            viper_pilot.set_num_episodes(rl_utils.num_episodes)            viper_pilot.set_q_cache(q_cache)            viper_pilot.set_log_file(f)            viper_pilot.set_global_log_file(global_log)            viper_pilot.set_e(e)                        # instead of calling super(RLMultiAgentSimulation, self).run(), duplicate and modify the code        # so we can terminate an episode if agent is in terminal state        self.current_time = self.tmin        count = 0#             self.viper.fcs.platform.x = 50920#             self.viper.fcs.platform.v = 135        while count < rl_utils.max_ticks: #not self.umpire.check_termination_triggers() :            count += 1 # prevent from getting stuck            if isinstance(self.viper.pilot, BaseAgent):                self.viper.pilot.set_timestep(count)            self.current_time = round(self.current_time, 1)            self.dt = round(self.dt, 1)                        self.tick(self.current_time, self.dt)            self.timestep += 1            self.current_time += self.dt            self.umpire.evaluate_agent_performance()                        state = self.viper.pilot.prev_state#             if hasattr(self.viper.pilot, 'in_goal') and self.viper.pilot.in_goal == rl_utils.MIN_NUM_IN_GOAL:#                 print "reached goal at t "            if self.viper.pilot.consecutive_in_goal >= rl_utils.MIN_NUM_IN_GOAL:                print('Reached goal at t ', self.current_time, " and continued")                #break                        if self.viper.pilot.is_terminal(state):                print("break state (learning): " + state)                break                        f.close()                #print "viper type:", type(self.viper) -> <class 'fighter.Fighter'>        #print "cobra type:", type(self.cobra) -> <class 'fighter.Fighter'>        #print "viper.fighter type:", type(self.viper.pilot) -> <class 'rlagents.agent.simple.RLSimpleAgent'>        #print "cobra.fighter type:", type(self.cobra.pilot) -> <class 'agents.fsm_agent.stern_conversion.SternConversionAgent'>        #print "viper sensor:", self.viper.sensor.name -> viper_sensor#             print "viper beliefs type:", type(self.viper.pilot.beliefs) -> agents.goal_agent.pilot.PilotBeliefs#             print "cobra beliefs type:", type(self.cobra.pilot.beliefs) -> agents.goal_agent.pilot.PilotBeliefs#             print "viper beliefs:", self.viper.pilot.beliefs.entity_state#             print "viper beliefs:", self.viper.pilot.beliefs.threat_state        #self.viper.pilot.Q = self.Q        #print "=================================="                    def run_test(self):        print("RLSimulation.run_test()")        scenario_file = rl_utils.scenario.split('/')[-1]        q_dir = 'rl_results/' + scenario_file + "/" + str(rl_utils.num_episodes).zfill(8) \                + "-" + str(rl_utils.MIN_NUM_IN_GOAL).zfill(4)        random_seed = '0'        q_filepath = q_dir + '/Q-' + random_seed                output_dir = 'rl_results/' + scenario_file + "/independent-tests"        if not os.path.exists(output_dir):            os.makedirs(output_dir)        random_seed = "0"        viper_trace = None        cobra_trace = None        test_only = True        q_cache = QCache()        # support for eligibility traces        e = {}        e.fromkeys(range(4000000))        self.timestep = 0        self.q = q_cache.get("q")        num_episodes = 1        q_cache.set_data(pickle.load(open(q_filepath)))                start_time = time.time()        global_log = open('global-log.txt', 'w')        print("====== Testing only")        test_result_filename = output_dir + "/test-result.txt"        if self.test(1, q_cache, test_result_filename, global_log):            pass #break        end_time = time.time()        duration = end_time - start_time        print("--------------------------------------------------------------------\n ---- Total duration (learning+activity): ", duration)        print("Total timesteps (activity only): ", self.timestep)    def test(self, episode, q_cache, test_result_filename, global_log):        print("test result file:", test_result_filename)        f = open(test_result_filename, 'w')        f.write("test result created at " + str(datetime.datetime.now()) + '\n')        f.write("scenario:" + str(rl_utils.scenario) + '\n')        f.write('min_num_in_goal:' + str(rl_utils.MIN_NUM_IN_GOAL) + "\n")        f.write('test conducted after episode ' + str(episode) + '\n')        f.write('episodes:' + str(rl_utils.num_episodes) + '\n')        f.write('total timesteps:' + str(self.timestep) + '\n\n')        f.write('t,blue.x, blue.y, blue.psi, red.x, red.y, red.psi, dx,state,action,mySpeed,enemySpeed\n')                    self.scenario['dt'] = rl_utils.dt        self.setup(self.scenario)        if isinstance(self.viper.pilot, BaseAgent):            viper_pilot = self.viper.pilot            viper_pilot.set_episode(episode)            viper_pilot.set_num_episodes(rl_utils.num_episodes)            viper_pilot.set_q_cache(q_cache)            viper_pilot.set_log_file(f)            viper_pilot.set_global_log_file(global_log)            self.viper.pilot.set_testing(True)        rl_utils.EPSILON = 0.0                        # instead of calling super(RLMultiAgentSimulation, self).run(), duplicate and modify the code        # so we can terminate an episode if agent is in terminal state        self.current_time = self.tmin        num_in_goal = 0        count = 0        test_successful = False        while count < 50000: #not self.umpire.check_termination_triggers() :            count += 1 # prevent from getting stuck            if isinstance(self.viper.pilot, BaseAgent):                self.viper.pilot.set_timestep(count)            self.current_time = round(self.current_time, 1)            self.dt = round(self.dt, 1)            self.tick(self.current_time, self.dt)            self.current_time += self.dt            self.umpire.evaluate_agent_performance()                        state = self.viper.pilot.prev_state            if self.viper.pilot.is_in_goal(state):                num_in_goal += 1                if num_in_goal == rl_utils.MIN_NUM_IN_GOAL:                    success_msg = "Goal reached at t=" + str(self.current_time)                    f.write('\n' + success_msg)                    print (success_msg)                    test_successful = True                    break            else:                num_in_goal = 0            in_terminal_state = False            if isinstance(self.viper.pilot, EligibilityTracesCQL2D1Agent):                if self.viper.pilot.is_terminal(state):                    in_terminal_state = True            else:                        # when activity 1D agent, do not use the agent's is_terminal                # speed must be declared even though not used                zone, speed = rl_utils.get_zone_and_speed(state)                if zone == 0 or zone > 130:                    in_terminal_state = True            if in_terminal_state:                fail_msg = "Test stopped after arriving in state " + state                f.write('\n' + fail_msg)                print(fail_msg)                break        f.close()        return test_successful    def tick(self, t, dt):        """        Ticks each fighter aircraft.        Gets the current state of each fighter and then passes each state to the        opponent aircraft. Once this is done, each aircraft is then ticked.                viper (blue) is our RL fighter        cobra (red) is the adversary        """        viper_state = self.viper.get_state()        cobra_state = self.cobra.get_state()        self.viper.update_adversary_state(cobra_state)        self.cobra.update_adversary_state(viper_state)        self.viper.tick(t, dt)        self.cobra.tick(t, dt)            def serialize(self, obj, path):        binary = open(path, mode='wb')        pickle.dump(obj, binary)        binary.close()            def deserialize(self, path):        return pickle.load(open(path))        def after_tick(self, tick):        f = self.state46        q = self.q        f.write(str(tick))        for speed in range(5):            state_action = "46-" + str(speed) + "-0"            value = q[state_action] if state_action in q else 0            f.write(',' + str(value))            state_action = "46-" + str(speed) + "-1"            value = q[state_action] if state_action in q else 0            f.write(',' + str(value))        pass        f.write('\n')