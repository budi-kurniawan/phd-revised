__author__ = 'bkurniawan'import pickleimport numpy as np
"""This class represents an actor-critic with traces agent"""class ActorCriticAgent:    ALPHA_THETA = 0.5    ALPHA_W = 0.5    GAMMA = 0.95    LAMBDA_THETA = 0.9    LAMBDA_W = 0.8        def __init__(self, num_state_vars: int, num_actions: int):        #super(Agent, self).__init__()        self.num_states = num_state_vars        self.num_actions = num_actions        self.theta = np.zeros([num_state_vars, num_actions], dtype=np.float64)        self.w = np.zeros(num_state_vars, dtype=np.float64)        self.z_theta = np.zeros([num_state_vars, num_actions], dtype=np.float64)        self.z_w = np.zeros(num_state_vars, dtype=np.float64)        self.actions = np.arange(num_actions)    def init(self, episode_no: int, num_episodes: int) -> None:        # reset traces        self.z_theta[:] = 0.0        self.z_w[:] = 0.0    def clean_up(self, episode_no: int, num_episodes: int) -> None:        pass            def softmax(self, x):        e_x = np.exp(x - np.max(x))        return e_x / e_x.sum(axis=0)     def select_action(self, discrete_state: int) -> int:        # episode_no and num_episodes are not used here but may be used in child classes        prob = self.softmax(self.theta[discrete_state])        return np.random.choice(self.actions, p=prob)    def decay_traces(self, z_theta, z_w):        z_theta *= ActorCriticAgent.LAMBDA_THETA        z_w *= ActorCriticAgent.LAMBDA_W    def update_weights(self, rhat, theta, w, z_theta, z_w):        w += ActorCriticAgent.ALPHA_W * rhat * z_w        theta += ActorCriticAgent.ALPHA_THETA * rhat * z_theta    def step(self, discrete_state, action, next_discrete_state, reward, terminal) -> None:        old_prediction = self.w[discrete_state]        prediction = 0.0 if terminal else self.w[next_discrete_state]        delta = reward + ActorCriticAgent.GAMMA * prediction - old_prediction        if not terminal:            self.z_theta[discrete_state][action] += 0.05 # strengthen the trace for the current state            self.z_w[discrete_state] += 0.2 # strengthen the trace for the current state            self.decay_traces(self.z_theta, self.z_w)        self.update_weights(delta, self.theta, self.w, self.z_theta, self.z_w)            def save_policy(self, path):        file = open(path,'wb')        pickle.dump(self.theta, file)        pickle.dump(self.w, file)        file.close()    def get_policy(self):        return self.theta