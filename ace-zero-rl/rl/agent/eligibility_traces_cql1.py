__author__ = 'bkurniawan'from .. import rl_utilsfrom rl.agent.eligibility_traces import EligibilityTracesAgent
class EligibilityTracesCQL1Agent(EligibilityTracesAgent):    ### An agent implementing eligibility traces and coarse Q-learning    def __init__(self, params_filename=None):        super(EligibilityTracesCQL1Agent, self).__init__()        self.t = 0        self.same_state_count = 0
    actions = [ 'no_command()', 'change_speed_by_percentage(-10)', 'change_speed_by_percentage(-20)', 'change_speed_by_percentage(-50)',               'change_speed_by_percentage(10)', 'change_speed_by_percentage(20)', 'change_speed_by_percentage(50)']
    def learn(self, t, dt):        action = None        reward = None        q = None        e = None        current_state = self.current_state        if self.prev_state == None:            action = self.get_random_action()        else:            reward = self.get_reward(current_state)            self.calculate_t(reward)            # this is the feature of CQL: Use the same action if the previous action brought the agent to the same current_state            if self.prev_state == current_state and self.same_state_count < 10:                action = self.prev_action                self.same_state_count += 1            else:                self.same_state_count = 0                q = self.get_q()                e = self.get_e()                prev_state_action = self.prev_state + "-" + str(self.prev_action)                # increment e(s,a)                self.increment_eligibility_trace(e, prev_state_action)                action, explore = self.get_explore_exploit_action(self.get_effective_epsilon(self.EPSILON), q, current_state)                greedy_action = action if not explore else self.get_action_with_max_q_value(q, current_state)                greedy = action == greedy_action                q1 = self.get_table_value(q, current_state + "-" + str(greedy_action))                q0 = self.get_table_value(q, prev_state_action)                delta = self.t + EligibilityTracesAgent.GAMMA * q1 - q0                # the following for loop + if are more efficient than the above                for state_action in e:                    # state_action is an existing key in e, no need to call get_table_value                    e_value = e[state_action]                    q_update = EligibilityTracesAgent.ALPHA * delta * e_value                    # delta may be 0, so q_update may be 0                    if q_update != 0.0:                        q[state_action] = self.get_table_value(q, state_action) + q_update                    if greedy:                        e[state_action] = self.GAMMA * self.LAMBDA * e_value                if not greedy:                    e.clear()        self.prev_state = current_state        self.prev_action = action        self.log(t, reward, current_state + '-' + str(action), q, e)        self.execute_action(action)        def calculate_t(self, reward):        self.t += reward