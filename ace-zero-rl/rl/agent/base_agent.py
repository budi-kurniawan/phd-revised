__author__ = 'bkurniawan'from ace_zero_core.agents.agent import Agentfrom ace_zero_core.pilot_commands import *import mathfrom random import random, randintfrom .. import rl_utils
class BaseAgent(Agent):    """    This class provides useful methods for agents    """    actions = []    def __init__(self, params_filename=None):        super(Agent, self).__init__()        self.prev_state = None        self.prev_action = None
    # how many consecutive times the agent has been in a goal zone     consecutive_in_goal = 0    def set_q_cache(self, q_cache):        self.q_cache = q_cache
    # support for eligibility traces    def set_e(self, e):        self.e = e            def set_log_file(self, f):        self.log_file = f            def set_global_log_file(self, f):        self.global_log_file = f            def set_episode(self, episode):        self.episode = episode    def set_num_episodes(self, num_episodes):        self.num_episodes = num_episodes            # Return a decaying epsilon    def get_effective_epsilon(self, epsilon):        context = rl_utils.context        return (context.num_episodes - context.episode) * epsilon / (context.num_episodes - 1)        def set_timestep(self, ts):        self.timestep = ts    def set_testing(self, flag):        self.testing = flag                def am_i_tracking(self):        tracks = self.beliefs.entity_state.sensor_state.tracks        callsigns = [tracks[t].callsign for t in tracks]         adversary_callsign = self.beliefs.threat_state.callsign        tracking = adversary_callsign in callsigns        total_time = 0 if not tracking else tracks[adversary_callsign].total_time        return tracking, total_time            def am_i_being_tracked(self):        return self.beliefs.entity_state.tracked        def turn(self, psi_c):        my_state = self.beliefs.entity_state        self.commands.append(SetHeadingGLoadCmd(psi_c=psi_c + my_state.psi_c, gload_c=5))            def get_my_tracks(self):        return self.beliefs.entity_state.sensor_state.tracks    def get_adversary_callsign(self):        return self.beliefs.threat_state.callsign        def get_adversary_tracks(self):        return self.beliefs.threat_state.sensor_state.tracks
    def get_my_callsign(self):        return self.beliefs.entity_state.callsign        def increase_speed(self):        max_speed = self.beliefs.entity_state.v_max        current_speed = self.beliefs.entity_state.v        self.commands.append(SetSpeedCmd(v_c=min(current_speed + 10, max_speed)))    def decrease_speed(self):        min_speed = self.beliefs.entity_state.v_min        current_speed = self.beliefs.entity_state.v        self.commands.append(SetSpeedCmd(v_c=max(current_speed - 10, min_speed)))    def increase_speed_based_on_enemy(self):        max_speed = self.beliefs.entity_state.v_max        enemy_speed = self.beliefs.threat_state.v        self.commands.append(SetSpeedCmd(v_c=min(enemy_speed + 10, max_speed)))    def decrease_speed_based_on_enemy(self):        min_speed = self.beliefs.entity_state.v_min        enemy_speed = self.beliefs.threat_state.v        self.commands.append(SetSpeedCmd(v_c=max(enemy_speed - 10, min_speed)))    def change_speed_by_percentage(self, delta):        self.speed_pct(delta)    def speed_pct(self, delta): #increase/decrease speed by delta%        max_speed = self.beliefs.entity_state.v_max        current_speed = self.beliefs.entity_state.v        new_speed = (1 + delta/100.0) * current_speed        self.commands.append(SetSpeedCmd(v_c=min(new_speed, max_speed)))            def set_speed(self, speed):        max_speed = self.beliefs.entity_state.v_max        min_speed = self.beliefs.entity_state.v_min        new_speed = max(min(max_speed, speed), min_speed)        self.commands.append(SetSpeedCmd(v_c=new_speed))            def set_maximum_speed(self):        max_speed = self.beliefs.entity_state.v_max        self.commands.append(SetSpeedCmd(v_c=max_speed))        def set_minimum_speed(self):        min_speed = self.beliefs.entity_state.v_min        self.commands.append(SetSpeedCmd(v_c=min_speed))            def match_speed(self, delta):        enemy_speed = self.beliefs.threat_state.v        self.commands.append(SetSpeedCmd(v_c=enemy_speed + delta))        def set_heading(self, delta):        my_heading = self.beliefs.entity_state.heading        new_heading = my_heading + delta        self.commands.append(SetHeadingGLoadCmd(psi_c=new_heading, gload_c=5))            def altitude(self, delta):        my_theta_c = self.beliefs.entity_state.theta_c        new_theta_c = my_theta_c + delta        new_theta_c = min(30, max(-30, new_theta_c))        self.commands.append(SetPitchAngleCmd(theta_c=new_theta_c))        def no_command(self):        pass    def get_my_speed(self):        my_state = self.beliefs.entity_state        return my_state.v        def get_adversary_speed(self):        threat_state = self.beliefs.threat_state        return threat_state.v        def get_my_minimum_speed(self):        return self.beliefs.entity_state.v_min    def get_my_maximum_speed(self):        return self.beliefs.entity_state.v_max        def get_my_commanded_speed(self):        my_state = self.beliefs.entity_state        return my_state.v_c            def get_distance(self):        my_state = self.beliefs.entity_state        threat_state = self.beliefs.threat_state        x1 = my_state.x        y1 = my_state.y        x2 = threat_state.x        y2 = threat_state.y        return math.sqrt(math.pow(x2 - x1, 2) + math.pow(y2 - y1, 2))    def get_relative_angle(self):        my_state = self.beliefs.entity_state        threat_state = self.beliefs.threat_state        return my_state.psi_c - threat_state.psi_c    def get_relative_bearing(self):        own_x = self.beliefs.entity_state.x        own_y = self.beliefs.entity_state.y        other_x = self.beliefs.threat_state.x
        other_y = self.beliefs.threat_state.y
    def get_table_value(self, table, state_action):        return table[state_action] if state_action in table else 0        def get_max_table_value(self, q, state):        # dict.get() is very slow, avoid that!        max_value = None        state_action = None        for i in range(len(self.actions)):            state_action = state + "-" + str(i)            value = q[state_action] if state_action in q else 0            if max_value == None or value > max_value:                max_value = value        return max_value, state_action;        def get_random_action(self):        return randint(0, len(self.actions) - 1)        def get_action_with_max_q_value(self, q, state):        max_value = None        temp = [] # contains all action indices with max value        for i in range(len(self.actions)):            key = state + "-" + str(i)            value = q[key] if key in q else 0            if max_value == None:                max_value = value                temp.append(i)            elif value == max_value:                temp.append(i)            elif value > max_value:                del temp[:] #empty temp                max_value = value                temp.append(i)        if len(temp) == 0:            return self.get_random_action()        elif len(temp) == 1:            return temp[0]        random_action_index = self.get_random_action_index(0, len(temp) - 1)        if random_action_index >= len(temp):            print("something wrong. random_action_index:", random_action_index, ", len(temp):", len(temp))        return temp[random_action_index]        def get_random_action_index(self, min_inc, max_inc):        return randint(min_inc, max_inc)        def get_explore_exploit_action(self, epsilon, q, state):        if random() < epsilon:            return self.get_random_action(), True        else:            return self.get_action_with_max_q_value(q, state), False            def is_in_goal(self, state):        zone, speed = rl_utils.get_zone_and_speed(state)        return zone >= 9 and zone <=46        def is_terminal(self, state):        zone = rl_utils.get_zone(state)        return zone > 130 or zone==0        def get_state(self, entity_state, threat_state):        # goldilock is 9 <= i <= 46        dx = entity_state.x - threat_state.x        i = int(int(dx) / 20)        if i < 0: # returns 0 if dx =< 0            i = 0        zone = str(i)        dv = entity_state.v - threat_state.v        speed = None        if dv < -15:            speed = "0" # we are very slow        elif dv < -5:            speed = "1" # slow        elif abs(dv) <= 5:            speed = "2" # good        elif dv > 15:            speed = "4" # very fast        else:            speed = "3" # fast        return zone + "-" + speed
    def execute_action(self, action):        exec('self.' + self.actions[action])    def tick(self, t, dt):        rl_utils.total_ticks += 1        s = self.beliefs.entity_state        context = rl_utils.context        context.t = t        if hasattr(context, 'log_file'):            log_file = context.log_file            log_file.write('t:' + str(t) + ', callsign:' + s.callsign + ', x:' + str(s.x) + ', y:' + str(s.y) + ', z:' + str(s.z) + ', z_c:' + str(s.z_c) +                       ', phi:' + str(s.phi) + ', phi_c:' + str(s.phi_c) + ', psi:' + str(s.psi) + ', psi_c:' + str(s.psi_c) +                        ', theta:' + str(s.theta) + ', theta_c:' + str(s.theta_c) +                       ', v:' + str(s.v) + ', v_c:' + str(s.v_c) + ', gload:' + str(s.gload) +                        ', prev_state:' + str(self.prev_state) + ', prev_action:' + str(self.prev_action) + "\n")        self.commands = []        my_state = self.beliefs.entity_state        threat_state = self.beliefs.threat_state        self.current_state = self.get_state(my_state, threat_state)        if self.is_in_goal(self.current_state):            self.consecutive_in_goal += 1        else:            self.consecutive_in_goal = 0                    if rl_utils.testing:            self.test(t, dt)        else:            self.learn(t, dt)        def learn(self, t, dt):        pass        def test(self, t, dt):        my_state = self.beliefs.entity_state        threat_state = self.beliefs.threat_state        state = self.get_state(my_state, threat_state)        q = self.get_q()        action = self.get_action_with_max_q_value(q, state)        if hasattr(rl_utils.context, 'log_file'):                    rl_utils.context.log_file.write(str.format("{0},({1},{2},{3},{4}),({5},{6},{7},{8}),{9},{10}\n",                     t, my_state.x, my_state.y, my_state.psi, my_state.v, threat_state.x, threat_state.y, threat_state.psi, threat_state.v,                     state, action))        self.prev_state = state        self.prev_action = action        self.execute_action(action)   